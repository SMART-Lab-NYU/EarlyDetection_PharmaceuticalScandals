{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPj__m4_a-O"
   },
   "source": [
    "# **1) Imports and connection to Google services**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21906,
     "status": "ok",
     "timestamp": 1631638031239,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "JZEdUFF5TCZ2",
    "outputId": "0a239bd3-e498-486b-ebd9-099a40298f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Collecting fr_core_news_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.7 MB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.62.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.6.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "***********************\n",
      "Completed\n",
      "***********************\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Code to read csv file into Colaboratory:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Installations \n",
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_sm\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import nltk\n",
    "import ssl\n",
    "import sys\n",
    "import spacy\n",
    "import fr_core_news_sm\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('french')\n",
    "nlp_fr = fr_core_news_sm.load()\n",
    "\n",
    "print('\\n***********************\\nCompleted\\n***********************') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRgtF26hFgdr"
   },
   "source": [
    "# **2) Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4201,
     "status": "ok",
     "timestamp": 1631638035423,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "XvmA7qSHFmVt",
    "outputId": "d3195df0-a382-4ea7-f1f5-ea855e3a84a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "File <dataset_doctissimo_22_03_2020.csv> has been loaded\n",
      "File <french_tweets.csv> has been loaded\n",
      "*****************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir = '/content/drive/MyDrive/Thèse Lévothyrox/2 JP Colab Notebooks/data/' # Files directory (in/out)\n",
    "\n",
    "# Data importing\n",
    "df_doctissimo = pd.read_csv(dir + 'dataset_doctissimo_22_03_2020.csv', encoding='utf8')\n",
    "print('\\n*****************\\nFile <dataset_doctissimo_22_03_2020.csv> has been loaded') \n",
    "df_french_tweets = pd.read_csv(dir + 'french_tweets.csv', encoding='utf8')\n",
    "print('File <french_tweets.csv> has been loaded\\n*****************\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J-mrSZLslIj"
   },
   "source": [
    "# **3) Functions & dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1631638035729,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "p1BBtceMso4K"
   },
   "outputs": [],
   "source": [
    "# Functions & Dictionaries\n",
    "def doctissimo_sort_range(df):\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "        df = df.sort_values(by=['date'], ascending=False)\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df = df.set_index(df['date'])\n",
    "        df = df[:'2015-12-31']\n",
    "        df['index'] = range(0, len(df))\n",
    "        df = df.set_index(df['index'])\n",
    "        del df['index']\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        return df\n",
    "\n",
    "def doctissimo_words_improvment(df):\n",
    "        df['text'] = df['text'].str.split() # Split string\n",
    "        i=0\n",
    "        for line in df['text']:\n",
    "                new_line = []\n",
    "                for word in line:\n",
    "                        for imprv in words_improvment:\n",
    "                                if word in imprv and word != imprv[0]:\n",
    "                                        word = imprv[0]\n",
    "                                        break\n",
    "                        new_line.append(word)\n",
    "                df.at[i, 'text'] = new_line\n",
    "                i += 1\n",
    "        df['text'] = df['text'].apply(' '.join) # Join string\n",
    "        return df\n",
    "\n",
    "def dataframe_preprocessing(df):\n",
    "        df['text'] = df['text'].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8') # Remove accent\n",
    "        df['text'] = [re.sub(r'[^a-zA-Z0-9 ]', ' ', str(x)) for x in df['text']] # Remove special characters and punctuation\n",
    "        df['text'] = df['text'].str.lower() # Convert ['text'] string to lowercase\n",
    "        df['text'] = df['text'].str.replace('#034', '', regex=True) # Remove #034 pattern\n",
    "        df['text'] = df['text'].str.replace('#039', '', regex=True) # Remove #039 pattern\n",
    "        df['text'] = df['text'].str.replace(\".*gif\", \"\", regex=True) # Remove all gifs\n",
    "        df['text'] = df['text'].str.replace(\"http.* \", \"\", regex=True) # Remove http links\n",
    "        df['text'] = df['text'].str.replace(\"https.* \", \"\", regex=True) # Remove https links\n",
    "        df['text'] = [re.sub(r'[A-Za-z]+\\d+|\\d+[A-Za-z]+','', str(x)) for x in df['text']] # Delete numbers between alphabetic chars\n",
    "        df['text'] = [re.sub(r'\\b(?!(\\D\\S*|[12][0-9]{3})\\b)\\S+\\b','', str(x)) for x in df['text']] # Numbers except dates\n",
    "        df['text'] = df['text'].str.replace('\\n',' ', regex=True).replace('\\t',' ', regex=True) # Remove line breaks and tabulations\n",
    "        df['text'] = [re.sub(r'(^| ).( |$)', ' ', str(x)) for x in df['text']] # Remove single characters\n",
    "        df['text'] = [re.sub(r'\\s+', ' ', str(x)) for x in df['text']] # Delete multiple spaces\n",
    "        return df\n",
    "\n",
    "def dataframe_stopwords_wtd(df):\n",
    "        # Words to delete (csv file)\n",
    "        words_to_delete = []\n",
    "        words_count = 0\n",
    "        with open(dir + 'exclusions.csv', newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            for row in reader:\n",
    "                words_to_delete.append('; '.join(row)) \n",
    "        words_count = len(words_to_delete)\n",
    "        words = df['text'].str.split()\n",
    "        words = words.apply(lambda x: [item for item in x if item not in words_to_delete]) # Words in \"text\" of dataframe) without excluded words\n",
    "        words = words.apply(lambda x: [item for item in x if item not in stop]) # From library \"FR\"\n",
    "        words = words.apply(lambda x: [item for item in x if item not in additional_stopwords]) # From dictionary\n",
    "        df['text'] = words.apply(' '.join)\n",
    "        return df\n",
    "\n",
    "def dataframe_lemmatization(df):\n",
    "        # Lemmatization with nlp_fr\n",
    "        df['text'] = df['text'].apply(lambda x: [y.lemma_ for y in nlp_fr(x)]).apply(' '.join) \n",
    "        return df\n",
    "\n",
    "def dataframe_duplicata_less3words(df):\n",
    "        words_count = df['text'].str.count(' ') + 1 # 'text' characters counter\n",
    "        df['words_count'] = words_count # Add words_count column on the dataframe\n",
    "        before_deleting = df['text'].count()\n",
    "        print('\\n******************\\nNumber of rows BEFORE deleting the rows which contains less than 3 words : ' + str(before_deleting) + ' rows')\n",
    "        df.drop(df[df['words_count'] < 3].index, inplace = True) # Remove rows which contains less than 3 words\n",
    "        after_deleting = df['text'].count()\n",
    "        print('Number of rows AFTER deleting the rows which contains less than 3 words : ' + str(after_deleting) + ' rows')\n",
    "        diff_deleting = before_deleting - after_deleting\n",
    "        print('Difference : ' + str(diff_deleting) + ' rows')\n",
    "        df.drop_duplicates() # Remove duplicates rows\n",
    "        df.dropna() # Drop the rows even with single NaN or single missing values.\n",
    "        after_del_duplicates = df['text'].count()\n",
    "        print('Number of rows after deleting duplicata : ' + str(after_del_duplicates) + ' rows')\n",
    "        diff_duplicates = after_deleting - after_del_duplicates\n",
    "        total_delete = diff_deleting + diff_duplicates\n",
    "        print('Difference : ' + str(diff_duplicates) + ' rows')\n",
    "        print('Total number of deleted rows : ' + str(total_delete) + '\\n******************')\n",
    "        return df\n",
    "\n",
    "def lemmatizer(text):        \n",
    "    sent = []\n",
    "    doc = nlp_fr(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)\n",
    "\n",
    "# StopWords dictionary\n",
    "additional_stopwords = ['a', 'abord', 'afin', 'ah', 'ai', 'ainsi', 'allaient', 'allo', 'allô', 'allons', 'alors', 'apres', 'après', 'assez', 'attendu', 'aucun', 'aucune', 'aucuns', 'aujourd', 'aujourdhui', 'auquel', 'auquelle', 'auquelles', 'auquels', 'aussi', 'autre', 'autres', 'auxquelles', 'auxquels', 'avant', 'avoir',\n",
    "                        'b', 'bonjour', 'bonsoir', 'bah', 'beaucoup', 'bien', 'bigre', 'bon', 'boum', 'br', 'bravo', 'brr', 'brrr',\n",
    "                        'ca', 'ça', 'car', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-la', 'celle-là', 'celles', 'celles-ci', 'celles-la', 'celles-là', 'celui', 'celui-ci', 'celui-la', 'celui-là', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'cet', 'cette', 'ceux', 'ceux', 'ceux-ci', 'ceux-là', 'ceux-là', 'chacun', 'chaque', 'cher', 'chere', 'chère', 'cheres', 'chères', 'chers', 'chez', 'chiche', 'chut', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantieme', 'cinquantième', 'cinquieme', 'cinquième', 'clac', 'clic', 'combien', 'comme', 'comment', 'compris', 'concernant', 'contre', 'couic', 'crac',\n",
    "                        'da', 'debout', 'debut', 'début', 'dedans', 'dehors', 'dela', 'delà', 'depuis', 'derriere', 'derrière', 'dés', 'dès', 'desormais', 'désormais', 'desquelles', 'desquels','dessous', 'dessus', 'deux', 'deuxieme', 'deuxième', 'deuxiemement', 'deuxièmement', 'devant', 'devers', 'devra', 'devrait', 'different', 'différent', 'differente', 'différente', 'differentes', 'différentes', 'differents', 'différents', 'dire', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', 'dix-sept', 'dixieme', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'douze', 'douzieme', 'douzième', 'dring', 'droite', 'duquel', 'durant',\n",
    "                        'e', 'effet', 'eh', 'elle-meme', 'elle-même', 'elles', 'elles-memes', 'elles-mêmes', 'encore', 'entre', 'envers', 'environ', 'ès', 'essai', 'etaient', 'etais', 'etait', 'etant', 'etante', 'etantes', 'etants', 'etat', 'état', 'etats', 'états', 'etc', 'ete', 'etee', 'etees', 'etes', 'etiez', 'etions', 'étions', 'etre', 'être', 'euh', 'eumes', 'eux-memes', 'eux-mêmes', 'excepte', 'excepté',\n",
    "                        'f', 'facon', 'façon', 'fais', 'faisaient', 'faisant', 'fait', 'faites', 'feront', 'fi', 'flac', 'floc', 'fois', 'font', 'force', 'fumes', 'futes',\n",
    "                        'g', 'gens',\n",
    "                        'h', 'ha', 'haut', 'he', 'hé', 'hein', 'helas', 'hélas', 'hem', 'hep', 'hi', 'ho', 'hola', 'holà', 'hop' ,'hormis', 'hors' ,'hou', 'houp', 'hue', 'hui', 'huit', 'huitieme', 'huitième', 'hum', 'hurrah',\n",
    "                        'i', 'ici', 'importe',\n",
    "                        'jusqu', 'jusqua', 'jusque', 'juste',\n",
    "                        'k',\n",
    "                        'là', 'laquelle', 'las', 'lequel', 'lès', 'lesquelles', 'lesquels', 'leurs', 'longtemps', 'lorsque', 'lui-meme', 'lui-même',\n",
    "                        'maint', 'maintenant', 'malgre', 'malgré', 'meme', 'memes', 'mêmes' ,'merci', 'mien', 'mienne', 'miennes', 'miens', 'mille', 'mince', 'mine', 'moi-meme', 'moi-même', 'moins', 'mot', 'moyennant',\n",
    "                        'na', 'nai', 'nas', 'neanmoins', 'néanmoins', 'neuf', 'neuvieme', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'nommes', 'nommés', 'non', 'nôtre', 'notres', 'nôtres', 'nous-meme', 'nous-memes', 'nous-memes', 'nous-mêmes', 'nouveau', 'nouveaux', 'nul',\n",
    "                        'o', 'onsoir', 'onjour', 'ô', 'oh', 'ohe', 'ohé', 'ole', 'olé', 'olle', 'ollé', 'onze', 'onzieme', 'onzième', 'ore', 'où', 'ouf', 'ouias', 'oust', 'ouste', 'outre',\n",
    "                        'p', 'paf', 'pan', 'parce', 'parmi', 'parmis', 'parole', 'partant', 'particulier', 'particuliere', 'particulière', 'particulierement', 'particulièrement', 'passe', 'passé', 'pendant', 'personne', 'personnes', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfff', 'pffff', 'pfft', 'pfut', 'piece', 'pièce', 'pif', 'plein', 'pleins', 'plouf', 'plupart', 'plus', 'plusieurs', 'plutot', 'plutôt', 'pouah', 'pourquoi', 'premier', 'premiere', 'première', 'premierement', 'premièrement', 'pres','près', 'proche', 'psitt', 'puisque',\n",
    "                        'q', 'quand', 'quant', 'quant-a-soi', 'quant-à-soi', 'quant-a-soit', 'quanta', 'quarante', 'quatorze', 'quatre', 'quatre-vingt', 'quatrieme', 'quatrième', 'quatriemement', 'quatrièmement', 'quel', 'quelconque', 'quell', 'quelle', 'quelle', 'quelles', 'quelles', 'quelque', 'quelques', 'quelquun', 'quels', 'quest', 'quiconque', 'quil', 'quils', 'quinze', 'quoi', 'quoique',\n",
    "                        'r', 'revoici', 'revoila', 'revoilà', 'rien',\n",
    "                        'sacrebleu', 'sans', 'sapristi', 'sauf', 'seize', 'selon', 'sept', 'septieme', 'seulement', 'si', 'sien', 'sienne', 'siennes', 'siens', 'sinon', 'six', 'sixieme', 'sixième', 'soi', 'soi-meme', 'soi-même', 'soient', 'sois', 'soixante', 'sous', 'suivant', 'sujet', 'surtout',\n",
    "                        'tac', 'tandis', 'tant', 'té', 'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tic', 'tien', 'tienne', 'tiennes', 'tiens', 'toc', 'toi-meme', 'toi-même', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutes', 'treize', 'trente', 'tres', 'très', 'trois', 'troisieme', 'troisième', 'troisiemement', 'troisièmement', 'trop', 'tsoin', 'tsouin',\n",
    "                        'u', 'unes', 'uns',\n",
    "                        'v', 'va', 'vais', 'valeur', 'valeurs', 'vas', 've', 'vé', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', 'vlan', 'voici', 'voie', 'voient', 'voila', 'voilà', 'vont', 'vôtre', 'votres', 'vôtres', 'vous-memes', 'vous-mêmes', 'vu', \n",
    "                        'w',\n",
    "                        'x',\n",
    "                        'z', 'zut']\n",
    "\n",
    "# Words_improvment dictionary\n",
    "words_improvment = [['levothyrox', 'levo', 'levothyro', 'levotyrox'],\n",
    "                    ['euthyrox', 'leuthyrox', 'eutyrox', 'leutyrox'],\n",
    "                    ['lthyroxine', 'lthyroxin', 'ltyroxine', 'ltyroxin'],\n",
    "                    ['hypothyroidie', 'lhypothyroidie', 'hypotyroidie', 'lhypotyroidie'],\n",
    "                    ['comprime', 'comprim'],\n",
    "                    ['cytomel', 'cynomel'],\n",
    "                    ['controle', 'control'],\n",
    "                    ['changer', 'change', 'chang', 'changement'],\n",
    "                    ['allemagne', 'allemand'],\n",
    "                    ['generaliste', 'generalist'],\n",
    "                    ['arret', 'arreter', 'arrete'],\n",
    "                    ['excipient', 'excipients'],\n",
    "                    ['laboratoire', 'laboratoir'],\n",
    "                    ['poids', 'poid'],\n",
    "                    ['hormone', 'hormon', 'dhormone', 'dhormon', 'lhormone', 'lhormon'],\n",
    "                    ['correcte', 'correct'],\n",
    "                    ['courche', 'coucher', 'chouchee'],\n",
    "                    ['neomercazole', 'neomercazol'],\n",
    "                    ['enceinte', 'enceint'],\n",
    "                    ['ancien', 'ancienne', 'lancien', 'lancienne'],\n",
    "                    ['francais', 'fraincaise', 'francai'],\n",
    "                    ['manque', 'manqu'],\n",
    "                    ['medicament', 'medicaments', 'medoc', 'medocs'],\n",
    "                    ['stress', 'stres'],\n",
    "                    ['analyse', 'danalyse'],\n",
    "                    ['hasimoto', 'dhashimoto', 'hasimoto'],\n",
    "                    ['thyroidite', 'thyroidit', 'tyroidite', 'tyroidit'],\n",
    "                    ['angoisse', 'dangoisse'],\n",
    "                    ['hypo', 'lhypo', 'dhypo'],\n",
    "                    ['hyper', 'lhyper', 'dhyper'],\n",
    "                    ['euthyral', 'leuthyral', 'deuthyral', 'eutyral', 'leutyral', 'deutyral'],\n",
    "                    ['autre', 'lautre'],\n",
    "                    ['pmol', 'pmoil'],\n",
    "                    ['soucis', 'souci'],\n",
    "                    ['augmente', 'augment', 'augmenter', 'daugmenter', 'daugmente', 'daugment'],\n",
    "                    ['sentais', 'sentai'],\n",
    "                    ['pensezvous', 'pensezvou'],\n",
    "                    ['echographie','echo', 'lecho', 'lechographie'],\n",
    "                    ['aller', 'alle', 'allee'],\n",
    "                    ['adenomegalie', 'dadenomegalie'],\n",
    "                    ['jaurai', 'jaurais'],\n",
    "                    ['elever', 'eleve', 'elevee'],\n",
    "                    ['cheveux', 'cheveu'],\n",
    "                    ['devrai', 'devrais'],\n",
    "                    ['menopause', 'menopaus'],\n",
    "                    ['nodule', 'nodul'],\n",
    "                    ['reactive', 'reactiv'],\n",
    "                    ['periode', 'period'],\n",
    "                    ['epuiser', 'epuisee', 'epuise'],\n",
    "                    ['arriver', 'arrive', 'narrive', 'narriv'],\n",
    "                    ['memoire', 'memoir'],\n",
    "                    ['parcours', 'parcour'],\n",
    "                    ['message', 'messages'],\n",
    "                    ['specialiste', 'specialist'],\n",
    "                    ['apriori', 'priori'],\n",
    "                    ['delais', 'delai'],\n",
    "                    ['gonfler', 'gonfle', 'gonflee'],\n",
    "                    ['ovaire', 'lorvair', 'lovair'],\n",
    "                    ['precis', 'preci'],\n",
    "                    ['prend', 'prends'],\n",
    "                    ['fatiguer', 'fatigue', 'fatiguee'],\n",
    "                    ['deprimer', 'deprime', 'deprimee', 'deprim'],\n",
    "                    ['penser', 'pense', 'pensee', 'pensez'],\n",
    "                    ['secretaire', 'secretair'],\n",
    "                    ['quelque', 'quelques', 'quelqu'],\n",
    "                    ['cause', 'caus'],\n",
    "                    ['lobe', 'lob'],\n",
    "                    ['t3', 't3l', 'ft3'],\n",
    "                    ['t4', 't4l', 'ft4'],\n",
    "                    ['norme', 'norm'],\n",
    "                    ['doser', 'dosee', 'dose'],\n",
    "                    ['endocrinologue', 'lendocrinologue', 'endocrino', 'lendocrino'],\n",
    "                    ['continu', 'continue'],\n",
    "                    ['interval', 'intervalle'],\n",
    "                    ['thyroidien', 'tyroidien'],\n",
    "                    ['soir', 'soiree'],\n",
    "                    ['conseil', 'conseille'],\n",
    "                    ['anticorps', 'anticorp'],\n",
    "                    ['ablation', 'lablation'],\n",
    "                    ['aider', 'aide', 'maide', 'maider'],\n",
    "                    ['ordre', 'ordr', 'lordre', 'lordr'],\n",
    "                    ['operation', 'loperation'],\n",
    "                    ['remercier', 'remercie'],\n",
    "                    ['marseille', 'marseill']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw3FxHFkWaOZ"
   },
   "source": [
    "# **4) Doctissimo dataset processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114881,
     "status": "ok",
     "timestamp": 1631638150608,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "wKyxg5clWUL2",
    "outputId": "d8c581f6-9902-45bb-a9d8-d83f64edccce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************\n",
      "Sorting and selecting range : 2016-2020... check\n",
      "*****************\n",
      "\n",
      "*****************\n",
      "Data cleaning and formating... check\n",
      "*****************\n",
      "\n",
      " l equivalent de serait au pifometre parce que personne ne le sait au juste mais meme si est exactement le bon dosage ce est pas du tout la meme chose la tu la sens tout de suite et elle est vite resortie la tu commences la sentir au bout de deux semaines et as effet complet au bout de semaines peut etre c est dire si demain ou dans quelques jours tu as plus de et tu commences tu tomberais dans un enorme trou parce tu aurais pratiquement rien pendant deux semaines je ai pas de medecin au maroc sous la main je vais voir si je peux me renseigner pour la au maroc oui et comment ca se fait que tu as un traitement de seul \n",
      "\n",
      "*****************\n",
      "Words improvment... check\n",
      "*****************\n",
      "\n",
      "l equivalent de serait au pifometre parce que personne ne le sait au juste mais meme si est exactement le bon dosage ce est pas du tout la meme chose la tu la sens tout de suite et elle est vite resortie la tu commences la sentir au bout de deux semaines et as effet complet au bout de semaines peut etre c est dire si demain ou dans quelque jours tu as plus de et tu commences tu tomberais dans un enorme trou parce tu aurais pratiquement rien pendant deux semaines je ai pas de medecin au maroc sous la main je vais voir si je peux me renseigner pour la au maroc oui et comment ca se fait que tu as un traitement de seul\n",
      "\n",
      "*****************\n",
      "Removing stop words and WTD... check\n",
      "*****************\n",
      "\n",
      "equivalent pifometre sait exactement dosage resortie commences sentir semaines complet semaines jours commences tomberais enorme trou pratiquement semaines medecin maroc main renseigner maroc traitement\n",
      "\n",
      "*****************\n",
      "Lemmatization... check\n",
      "*****************\n",
      "\n",
      "equivalent pifometre savoir exactement dosage resortie commence sentir semaine complet semaine jour commence tomberai enorme trou pratiquement semaine medecin maroc main renseigner maroc traitement\n",
      "\n",
      "******************\n",
      "Number of rows BEFORE deleting the rows which contains less than 3 words : 7583 rows\n",
      "Number of rows AFTER deleting the rows which contains less than 3 words : 6773 rows\n",
      "Difference : 810 rows\n",
      "Number of rows after deleting duplicata : 6773 rows\n",
      "Difference : 0 rows\n",
      "Total number of deleted rows : 810\n",
      "******************\n",
      "\n",
      "*****************\n",
      "Removing duplicates and rows which contains less than 3 words... check\n",
      "*****************\n",
      "\n",
      "\n",
      "File <dataset_doctissimo_updated_all_cleaning_steps.csv> has been exported\n",
      "Elapsed time - Doctissimo: 114.73746800422668 s\n",
      "\n",
      "            date             user  ...  year words_count\n",
      "index                              ...                  \n",
      "0     2020-03-21        freesia53  ...  2020          52\n",
      "2     2020-03-13    petitbouch​on  ...  2020          16\n",
      "3     2020-03-13     Susanne in F  ...  2020          24\n",
      "4     2020-03-13  NotYourMaj​esty  ...  2020          34\n",
      "5     2020-03-11          Clem120  ...  2020          31\n",
      "...          ...              ...  ...   ...         ...\n",
      "7578  2016-01-02     Smoothie27​7  ...  2016          42\n",
      "7579  2016-01-02          seb58dy  ...  2016          12\n",
      "7580  2016-01-01          glo18fz  ...  2016           6\n",
      "7581  2016-01-01          .Dr40av  ...  2016          10\n",
      "7582  2016-01-01          .Dr40av  ...  2016          16\n",
      "\n",
      "[6773 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Doctissimo : start\n",
    "start = time.time()\n",
    "df = df_doctissimo.copy()\n",
    "del df_doctissimo\n",
    "# Doctissimo : sort by date and select range : 2016-2020\n",
    "df = doctissimo_sort_range(df)\n",
    "print('*****************\\nSorting and selecting range : 2016-2020... check\\n*****************')\n",
    "# Doctissimo dataframe preprocessing\n",
    "df = dataframe_preprocessing(df)\n",
    "print('\\n*****************\\nData cleaning and formating... check\\n*****************\\n')\n",
    "print(df.iloc[3,2])\n",
    "# Doctissimo : words improvment\n",
    "df = doctissimo_words_improvment(df)\n",
    "print('\\n*****************\\nWords improvment... check\\n*****************\\n')\n",
    "print(df.iloc[3,2])\n",
    "# Doctissimo stop words removing\n",
    "df = dataframe_stopwords_wtd(df)\n",
    "print('\\n*****************\\nRemoving stop words and WTD... check\\n*****************\\n')\n",
    "print(df.iloc[3,2])\n",
    "# Doctissimo lemmatization (COMMENT TO ENHANCE TIME OF EXECUTION)\n",
    "df = dataframe_lemmatization(df)\n",
    "print('\\n*****************\\nLemmatization... check\\n*****************\\n')\n",
    "print(df.iloc[3,2])\n",
    "# Doctissimo final cleaning step\n",
    "df = dataframe_duplicata_less3words(df)\n",
    "print('\\n*****************\\nRemoving duplicates and rows which contains less than 3 words... check\\n*****************\\n')\n",
    "df.to_csv(dir + 'dataset_doctissimo_updated_all_cleaning_steps.csv', index=False, sep=',', header=True, encoding='utf8')\n",
    "print('\\nFile <dataset_doctissimo_updated_all_cleaning_steps.csv> has been exported')\n",
    "end = time.time()\n",
    "print('Elapsed time - Doctissimo:', end - start, 's')\n",
    "print()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1631638150609,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "QAzmpMXRIG1G",
    "outputId": "96fa8ffc-6623-410c-b91c-9dfa46ed0373"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>freesia53</td>\n",
       "      <td>suivre thyroidite hasimoto deconvenue levothyr...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2020</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>petitbouch​on</td>\n",
       "      <td>medecin prescrit hypothyroidie secondaire tsh ...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2020</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>Susanne in F</td>\n",
       "      <td>equivalent pifometre savoir exactement dosage ...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2020</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>NotYourMaj​esty</td>\n",
       "      <td>hypothyroidie traitement an quotidiennement mi...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2020</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>Clem120</td>\n",
       "      <td>date savoir fille atteinte syndrome interrupti...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2020</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>Smoothie27​7</td>\n",
       "      <td>endocrinologue prescrit prise sang verifie ver...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2016</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>seb58dy</td>\n",
       "      <td>converser atteinte thyroidit hashimoto traitem...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>glo18fz</td>\n",
       "      <td>montrent hypothyroidie fier tsh medicament tsh</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>.Dr40av</td>\n",
       "      <td>taux tsh hyperthyroidie falloir revoir medecin...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>.Dr40av</td>\n",
       "      <td>pareille commencer tsh levothyro tsh baisse le...</td>\n",
       "      <td>https://forum.doctissimo.fr/sante/thyroide-pro...</td>\n",
       "      <td>2016</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6773 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date             user  ...  year words_count\n",
       "index                              ...                  \n",
       "0     2020-03-21        freesia53  ...  2020          52\n",
       "2     2020-03-13    petitbouch​on  ...  2020          16\n",
       "3     2020-03-13     Susanne in F  ...  2020          24\n",
       "4     2020-03-13  NotYourMaj​esty  ...  2020          34\n",
       "5     2020-03-11          Clem120  ...  2020          31\n",
       "...          ...              ...  ...   ...         ...\n",
       "7578  2016-01-02     Smoothie27​7  ...  2016          42\n",
       "7579  2016-01-02          seb58dy  ...  2016          12\n",
       "7580  2016-01-01          glo18fz  ...  2016           6\n",
       "7581  2016-01-01          .Dr40av  ...  2016          10\n",
       "7582  2016-01-01          .Dr40av  ...  2016          16\n",
       "\n",
       "[6773 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1631638150610,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "rz7_D2LFGvss",
    "outputId": "d711b5c9-1869-4227-9f7e-dd8adcf86fde"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'medecin prescrit hypothyroidie secondaire tsh probleme falloir constituer stock correspondre prise assimilee faudra constituiez stock prendre'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l3Adbyukh68"
   },
   "source": [
    "# **5) French_tweets processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1631638153092,
     "user": {
      "displayName": "Valentin Roche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXkjdE9LxHFneNRhfMkPHUEGhjL4_1nyevYwl_dQ=s64",
      "userId": "04363626431418761298"
     },
     "user_tz": -120
    },
    "id": "OBGU_ceMHNRf",
    "outputId": "60e1f5e5-9345-438a-c0f5-3bd0fea63eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************\n",
      "All formating steps... check\n",
      "*****************\n",
      "\n",
      "******************\n",
      "Number of rows BEFORE deleting the rows which contains less than 3 words : 50 rows\n",
      "Number of rows AFTER deleting the rows which contains less than 3 words : 40 rows\n",
      "Difference : 10 rows\n",
      "Number of rows after deleting duplicata : 40 rows\n",
      "Difference : 0 rows\n",
      "Total number of deleted rows : 10\n",
      "******************\n",
      "\n",
      "Files <french_tweets_updated_all_cleaning_steps.csv> and <french_tweets_updated_all_cleaning_steps.txt> have been exported\n",
      "Elapsed time - French_tweets:  2.459805488586426 s\n",
      "\n",
      "            sentiment                                               text\n",
      "0   __label__negative                   awww bummer devrai david carr re\n",
      "1   __label__negative  contrarie pouvoir facebook telemaigner pleurer...\n",
      "2   __label__negative          plonge ball reussi economiser sort limite\n",
      "3   __label__negative                             corps demangeaison feu\n",
      "4   __label__negative                            comporte colere partout\n",
      "7   __label__negative                                   vue pleuvoir lol\n",
      "10  __label__negative            vacance printemps ville ordinaire neige\n",
      "12  __label__negative  pouvai supporter regarder pensai perte ua emba...\n",
      "14  __label__negative                    arme feu zac snyder doucheclown\n",
      "15  __label__negative                             aimerai regarde manque\n",
      "16  __label__negative  scene mort hollis humer severement regarder fi...\n",
      "18  __label__negative          ahh iv vouloir loyer adorer band original\n",
      "19  __label__negative                      buviez boisson table oubliees\n",
      "21  __label__negative             ami appele rencontrer mi vallee soupir\n",
      "22  __label__negative                           cuisine gateau apprendre\n",
      "25  __label__negative                    detest devoir appeler reveiller\n",
      "26  __label__negative                               crier regarde marley\n",
      "27  __label__negative                                  triste miss lilly\n",
      "28  __label__negative           ooooh lol leslie refuserai leslie facher\n",
      "29  __label__negative                     meh amer exception pist deprim\n",
      "30  __label__negative                            pirate but devoir creer\n",
      "31  __label__negative  promouvoir engrenage sillon bas pourrai anahei...\n",
      "32  __label__negative  dormir option realiser evaluation matin travai...\n",
      "33  __label__negative                                 craindre aim manqu\n",
      "34  __label__negative                  pleure oeil asiatique dormir nuit\n",
      "35  __label__negative  malade asseoir douche malade supporter retenir...\n",
      "36  __label__negative       raconte histoire tard bon travaillerai heure\n",
      "37  __label__negative                            pardon coucher venu gmt\n",
      "38  __label__negative               deprimer voulai savoir enfant valise\n",
      "39  __label__negative  lit classe travail gymnase classe petit amie m...\n",
      "40  __label__negative            envie lever doi etudier examen pratique\n",
      "41  __label__negative                               larme guitare casser\n",
      "42  __label__negative       triste triste triste detest sentiment dormir\n",
      "43  __label__negative                     awww souhait aise triste manqu\n",
      "44  __label__negative  endormir corps fille tracy tristete famille fa...\n",
      "45  __label__negative                      yay heureux travail signifier\n",
      "46  __label__negative  verifie chronologie utilisateur blackberry twa...\n",
      "47  __label__negative       mec top prefere repasser porter reunion brul\n",
      "48  __label__negative              etrangement triste lilo samro separer\n",
      "49  __label__negative                          desole reflechi retweeter\n"
     ]
    }
   ],
   "source": [
    "# French_tweets : start\n",
    "start = time.time()\n",
    "df = df_french_tweets.astype(str).head(50).copy()\n",
    "del df_french_tweets\n",
    "# French_tweets dataframe preprocessing\n",
    "df = dataframe_preprocessing(df)\n",
    "# French_tweets : labeling format\n",
    "df['sentiment'] = df['label']\n",
    "df['sentiment'] = df['sentiment'].str.replace(\"0\", \"negative\", regex=True)\n",
    "df['sentiment'] = df['sentiment'].str.replace(\"1\", \"positive\", regex=True)\n",
    "col = ['sentiment', 'text']\n",
    "df = df[col]\n",
    "df['sentiment']=['__label__'+ s for s in df['sentiment']]\n",
    "# French_tweets stop words removing\n",
    "df = dataframe_stopwords_wtd(df)\n",
    "# French_tweets lemmatization (COMMENT TO ENHANCE TIME OF EXECUTION)\n",
    "df = dataframe_lemmatization(df)\n",
    "print('*****************\\nAll formating steps... check\\n*****************')\n",
    "# French_tweets final cleaning step\n",
    "df = dataframe_duplicata_less3words(df)\n",
    "del df['words_count'] # Remove \"words_count\" column\n",
    "df.to_csv(dir + 'french_tweets_updated_all_cleaning_steps.csv', index=False, sep=',', header=True, encoding='utf8')\n",
    "df.to_csv(dir + 'french_tweets_updated_all_cleaning_steps.txt', index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "print('\\nFiles <french_tweets_updated_all_cleaning_steps.csv> and <french_tweets_updated_all_cleaning_steps.txt> have been exported')\n",
    "end = time.time()\n",
    "print('Elapsed time - French_tweets: ', end - start, 's')\n",
    "print()\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2. Datasets formating and cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
